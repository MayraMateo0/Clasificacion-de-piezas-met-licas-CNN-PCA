{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e46608e9"
      },
      "outputs": [],
      "source": [
        "from numpy import *\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import decomposition\n",
        "import time\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import keras\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD, Nadam, Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.preprocessing import image \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import sys\n",
        "from keras import optimizers\n",
        "\n",
        "from keras import backend as K\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.linalg import eigh\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "K.clear_session()\n",
        "\n",
        "from keras.layers.pooling import AveragePooling2D\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from imutils import paths\n",
        "\n",
        "\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import efficientnet.keras as enet\n",
        "\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, precision_score, recall_score, accuracy_score, roc_auc_score\n",
        "import seaborn as sns\n",
        " "
      ],
      "id": "e46608e9"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "apfg2bgroYGw"
      },
      "outputs": [],
      "source": [
        "#!pip install -U segmentation-models"
      ],
      "id": "apfg2bgroYGw"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BteoYkA4gUfj",
        "outputId": "9f4d853d-ea36-4534-84ba-80ff61e43edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "id": "BteoYkA4gUfj"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bbR7ZFtjhXkL"
      },
      "outputs": [],
      "source": [
        "dirname=os.path.join('/content/gdrive/MyDrive/Colab Notebooks/data/entrenamiento')"
      ],
      "id": "bbR7ZFtjhXkL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b19e49a1"
      },
      "outputs": [],
      "source": [
        "num = 1\n",
        "\n",
        "##########################crea vector tamaño num para los datos\n",
        "n = 0 ##sirve de contador\n",
        "\n",
        "datos = [0 for x in range(num)] ## \n",
        "\n",
        "dataAccuracy = [0 for x in range(num)]\n",
        "dataSensibilidad = [0 for x in range(num)]\n",
        "dataPrecision = [0 for x in range(num)]\n",
        "dF1 = [0 for x in range(num)]\n",
        "\n",
        "##########################\n",
        "for _ in itertools.repeat(None, num):\n",
        "  imgpath = dirname + os.sep \n",
        "  images = []\n",
        "  directories = []\n",
        "  dircount = []\n",
        "  prevRoot=''\n",
        "  cant=0\n",
        "  \n",
        "  print(\"leyendo imagenes de \",imgpath)\n",
        "\n",
        "  for root, dirnames, filenames in os.walk(imgpath):\n",
        "      for filename in filenames:\n",
        "          if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
        "              cant=cant+1\n",
        "              filepath = os.path.join(root, filename)\n",
        "              image = plt.imread(filepath)\n",
        "              images.append(image)\n",
        "              b = \"Leyendo...\" + str(cant)\n",
        "              print (b, end=\"\\r\")\n",
        "              if prevRoot !=root:\n",
        "                  print(root, cant)\n",
        "                  prevRoot=root\n",
        "                  directories.append(root)\n",
        "                  dircount.append(cant)\n",
        "                  cant=0\n",
        "  dircount.append(cant)\n",
        "  if len(dircount) > 1:\n",
        "      dircount = dircount[1:]\n",
        "      dircount[0] = dircount[0] + 1\n",
        "  print('Directorios leidos:',len(directories))\n",
        "  print(\"Imagenes en cada directorio\", dircount)\n",
        "  print('suma Total de imagenes en subdirs:',sum(dircount))\n",
        "\n",
        "  num_capas=64\n",
        "  optimizador='nadam'\n",
        "  componentes=32\n",
        "\n",
        "  labels=[]\n",
        "  indice=0\n",
        "  for cantidad in dircount:\n",
        "        for i in range(cantidad):\n",
        "            labels.append(indice)\n",
        "        indice=indice+1\n",
        "  print(\"Cantidad etiquetas creadas: \",len(labels))\n",
        "\n",
        "  deportes=[]\n",
        "  indice=0\n",
        "  for directorio in directories:\n",
        "      name = directorio.split(os.sep)\n",
        "      print(indice , name[len(name)-1])\n",
        "      deportes.append(name[len(name)-1])\n",
        "      indice=indice+1\n",
        "\n",
        "  y = np.array(labels)\n",
        "  X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
        "\n",
        " \n",
        "  classes = np.unique(y)\n",
        "  nClasses = len(classes)\n",
        "  print('Total number of outputs : ', nClasses)\n",
        "  print('Output classes : ', classes)\n",
        "\n",
        "  train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.4, train_size=0.6, random_state=13)\n",
        "  train_X = np.resize(train_X,(720,784))\n",
        "  test_X = np.resize(test_X,(720,784))\n",
        "\n",
        "  print(train_X.shape, train_Y.shape, test_X.shape)\n",
        "\n",
        "  train_X = train_X/255\n",
        "  test_X = test_X/255\n",
        "\n",
        "\n",
        "  standardized_scalar = StandardScaler()\n",
        "\n",
        "  standardized_data = standardized_scalar.fit_transform(train_X)\n",
        "  standardized_data.shape\n",
        "\n",
        "\n",
        "  cov_matrix = np.matmul(standardized_data.T, standardized_data)\n",
        "  cov_matrix.shape\n",
        "\n",
        "  lambdas, vectors = eigh(cov_matrix, eigvals=(782, 783))\n",
        "  vectors.shape\n",
        "\n",
        "  cov_mat = np.cov(standardized_data.T)\n",
        "\n",
        "  eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "  print('Eigenvectors \\n%s' %eig_vecs)\n",
        "  print('\\nEigenvalues \\n%s' %eig_vals)\n",
        "\n",
        "\n",
        "  #  Hacemos una lista de parejas (autovector, autovalor) \n",
        "  eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
        "\n",
        "  # Ordenamos estas parejas den orden descendiente con la función sort\n",
        "  eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  # Visualizamos la lista de autovalores en orden desdenciente\n",
        "  print('Autovalores en orden descendiente:')\n",
        "  for i in eig_pairs:\n",
        "      print(i[0])\n",
        "\n",
        "  # A partir de los autovalores, calculamos la varianza explicada\n",
        "  tot = sum(eig_vals)\n",
        "  var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
        "  cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "  # Representamos en un diagrama de barras la varianza explicada por cada autovalor, y la acumulada\n",
        "  with plt.style.context('seaborn-pastel'):\n",
        "      plt.figure(figsize=(10, 10))\n",
        "\n",
        "      plt.bar(range(784), var_exp, alpha=0.9, align='center',label='Varianza individual explicada', color='g')\n",
        "      plt.step(range(784), cum_var_exp, where='mid', linestyle='--', label='Varianza explicada acumulada')\n",
        "      plt.ylabel('Ratio de Varianza Explicada')\n",
        "      plt.xlabel('Componentes Principales')\n",
        "      plt.legend(loc='best')\n",
        "      plt.tight_layout()\n",
        "\n",
        "  #Generamos la matríz a partir de los pares autovalor-autovector\n",
        "  matrix_w = np.hstack((eig_pairs[0][1].reshape(784,1),\n",
        "                        eig_pairs[1][1].reshape(784,1)))\n",
        "\n",
        "  print('Matriz W:\\n', matrix_w)\n",
        "\n",
        "  Y = standardized_data.dot(matrix_w)\n",
        "\n",
        "  vectors = vectors.T\n",
        "  vectors.shape\n",
        "\n",
        "  train_X.shape\n",
        "\n",
        "  \n",
        "  new_coordinates = np.matmul(vectors, standardized_data.T)\n",
        "  print(new_coordinates.shape)\n",
        "\n",
        "  new_coordinates = np.vstack((new_coordinates, train_Y)).T\n",
        "  df_new = pd.DataFrame(new_coordinates, columns=[\"f1\", \"f2\", \"etiquetas\"])\n",
        "  df_new.head()\n",
        "  \n",
        "  \n",
        "\n",
        "  pca = decomposition.PCA()\n",
        "  pca.n_components = componentes\n",
        "  pca_data = pca.fit_transform(standardized_data)\n",
        "  percent_variance_retained = pca.explained_variance_ / np.sum(pca.explained_variance_)\n",
        "  \n",
        "  cum_variance_retained = np.cumsum(percent_variance_retained)\n",
        "  print(\"varianza acumulada\",cum_variance_retained)\n",
        "  plt.figure(2, figsize=(5, 3))\n",
        "  plt.clf()\n",
        "  plt.plot(cum_variance_retained, linewidth=2)\n",
        "  plt.axis(\"tight\")\n",
        "  plt.title(\"Gráfica de componentes en relación a la varianza \\n\", \n",
        "            fontdict={'family': 'serif', \n",
        "                      'color' : 'black',\n",
        "                      'weight': 'bold',\n",
        "                      'size': 14})\n",
        "  plt.grid()\n",
        "  plt.xlabel(\"Número de componentes\", fontsize=10)\n",
        "  plt.ylabel(\"Varianza\", fontsize=10)\n",
        "  plt.savefig(\"pca_cumulative_variance.png\")\n",
        "  plt.show()\n",
        "\n",
        "  train_X = np.array(train_X)\n",
        "  train_Y = np.array(train_Y)\n",
        "\n",
        "  plt.figure(figsize=(20,4))\n",
        "  for index, (image, label) in enumerate(zip(train_X[1:4], train_Y[1:4])):\n",
        "      plt.subplot(1, 5, index + 1)\n",
        "      plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
        "      plt.title('Proyección de características  \\n' % label, fontsize = 20)\n",
        "\n",
        "  train_X = train_X.reshape(train_X.shape[0], 28, 28, 1)\n",
        "  print(train_X.shape, train_Y.shape)\n",
        "\n",
        "  nclasses = train_Y.max() - train_Y.min() + 1\n",
        "  train_Y = to_categorical(train_Y, num_classes = nclasses)\n",
        "  print(\"Shape of ytrain after encoding: \",train_Y.shape)\n",
        "\n",
        "  train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.1)\n",
        "  print('Training data shape : ', train_X.shape, train_Y.shape)\n",
        "  print('Testing data shape : ', test_X.shape, test_Y.shape)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=[5,5])\n",
        "\n",
        "  plt.subplot(121)\n",
        "  plt.imshow(train_X[0,:,:], cmap='gray')\n",
        "  plt.title(\"img : {}\".format(train_Y[0]))\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.imshow(test_X[0,:,:], cmap='gray')\n",
        "  plt.title(\"img : {}\".format(test_Y[0]))\n",
        "\n",
        "  plt.figure(figsize=[5,5])\n",
        "\n",
        "\n",
        "\n",
        "  train_X = train_X.astype('float32')\n",
        "  test_X = test_X.astype('float32')\n",
        "  train_X = train_X / 255.\n",
        "  test_X = test_X / 255.\n",
        "  ###aq\n",
        "  batch_size = 1200\n",
        "  img_height = 150\n",
        "  img_width = 150\n",
        "\n",
        "  data_dir='/content/gdrive/MyDrive/Colab Notebooks/data/entrenamiento'\n",
        "  data_val='/content/gdrive/MyDrive/Colab Notebooks/data/validacion'\n",
        "  \n",
        "  \n",
        "  entrenamiento_datagen = ImageDataGenerator(\n",
        "\n",
        "      rotation_range=5,\n",
        "      rescale=1./255,        # Normalizar la imagen\n",
        "      shear_range=0.1,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True\n",
        "      \n",
        "      )\n",
        "\n",
        "  \n",
        "  test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "  test_datagen = ImageDataGenerator()\n",
        "  ds_entrena = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.001,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "    \n",
        "  ds_val = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_val,\n",
        "    validation_split=0.001,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "\n",
        "  for image_batch, labels_batch in ds_entrena:\n",
        "    print(image_batch.shape)\n",
        "    print(labels_batch.shape)\n",
        "    break\n",
        "\n",
        "\n",
        "  \n",
        "  #definir el modelo\n",
        "  num_classes = 6\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "\n",
        "    #tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
        "  tf.keras.layers.Conv2D(10, (4,4), activation='relu', input_shape=(150, 150,3)),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(4,4)),\n",
        "\n",
        "  tf.keras.layers.Conv2D(num_capas, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(784, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  ###compilar el modelo\n",
        "  model.compile(\n",
        "  optimizer=optimizador,\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        " \n",
        "\n",
        "  inicio = time.time()\n",
        "\n",
        "  history=model.fit(\n",
        "    ds_entrena,\n",
        "    validation_data=ds_val,\n",
        "    epochs=50\n",
        "  )\n",
        "\n",
        "  fin = time.time()\n",
        "  print('Tiempo:',fin-inicio)\n",
        "\n",
        "  target_dir = '/content/gdrive/MyDrive/Colab Notebooks/modelo_PCA/'\n",
        "  if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "  model.save('/content/gdrive/MyDrive/Colab Notebooks/modelo_PCA/modelo.h5')\n",
        "  model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/modelo_PCA/pesos.h5')\n",
        "\n",
        "  test_eval = model.evaluate(ds_entrena, verbose=5)\n",
        "\n",
        "  print('Test loss:', test_eval[0])\n",
        "  print('Test accuracy:', test_eval[1])\n",
        "\n",
        "\n",
        "  history_dict = history.history\n",
        "  print(history_dict.keys())\n",
        "\n",
        "  acc      = history.history[     'accuracy' ]\n",
        "  val_acc  = history.history[ 'val_accuracy' ]\n",
        "  loss     = history.history[    'loss' ]\n",
        "  val_loss = history.history['val_loss' ]\n",
        "  \n",
        "\n",
        "  epochs   = range(1,len(acc)+1,1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "  plt.plot ( epochs,     acc, 'r', label='Training data accuracy'  )\n",
        "  plt.plot ( epochs, val_acc,  'b', label='Validation data accuracy')\n",
        "  plt.title ('CNN+PCA training and validation')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot ( epochs,     loss, 'r', label='Loss of training data'  )\n",
        "  plt.plot ( epochs, val_loss,  'b', label='Loss of validation data')\n",
        "  plt.title ('CNN+PCA training and validation'   )\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "\n",
        "  acc=history.history['accuracy']\n",
        "  val_acc=history.history['val_accuracy']\n",
        "  loss=history.history['loss']\n",
        "  val_loss=history.history['val_loss']\n",
        "  epocas=range(len(acc))\n",
        "  #plt.plot(epocas,acc,'r',label='Precisión entrenamiento')\n",
        "  plt.plot(epocas,val_acc,'b',label='Validation accuracy')\n",
        "  plt.title('Training accuracy and validation')\n",
        "  plt.legend(loc=0)\n",
        "  plt.figure()\n",
        "  plt.show()\n",
        "\n",
        "  \n",
        "  get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "  # In[19]:\n",
        "\n",
        "  width_shape = 150\n",
        "  height_shape = 150\n",
        "  batch_size = batch_size\n",
        "\n",
        "  names = [ \"nail\", \"rectangle\", \"square\", \"washer\", \"circle\", \"nut\"]\n",
        "  \n",
        "\n",
        "\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/regulares/8M/2000lx\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/regulares/8M/1500lx_8M\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/regulares/8M/1000lx_8M\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/data/validacion\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/regulares/1.3M/1000lx\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/regulares/1.3M/1500lx\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/regulares/1.3M/2000lx\"\n",
        "\n",
        "\n",
        "\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/Irregulares/irregulares1.3M/1000lx\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/Irregulares/irregulares1.3M/1500lx\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/Irregulares/irregulares1.3M/2000lx\"\n",
        "\n",
        "\n",
        "\n",
        "  #test_data_dir=\"/content/gdrive/MyDrive/Colab Notebooks/mezcla_val_min/8M/1000lx_8M\"\n",
        "  #test_data_dir=\"/content/gdrive/MyDrive/Colab Notebooks/mezcla_val_min/8M/1500lx_8M\"\n",
        "  #test_data_dir=\"/content/gdrive/MyDrive/Colab Notebooks/mezcla_val_min/8M/2000lx\"\n",
        "\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/mezcla_val/1.3M/1000lx\"\n",
        "  test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/mezcla_val/1.3M/1500lx\"\n",
        "  #test_data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/mezcla_val/1.3M/2000lx\"\n",
        "\n",
        "  test_datagen = ImageDataGenerator()\n",
        "  ds_val = test_datagen.flow_from_directory(\n",
        "      test_data_dir,\n",
        "      target_size=(img_height, img_width), \n",
        "      batch_size = batch_size,\n",
        "      class_mode='categorical', \n",
        "      shuffle=False)\n",
        "\n",
        "\n",
        "  custom_Model= load_model(\"/content/gdrive/MyDrive/Colab Notebooks/modelo_PCA/modelo.h5\")\n",
        "\n",
        "  predictions = custom_Model.predict_generator(generator=ds_val)\n",
        "\n",
        "  y_pred = np.argmax(predictions, axis=1)\n",
        "  y_real = ds_val.classes\n",
        "\n",
        "  print(y_real)\n",
        "  print(y_pred)\n",
        "\n",
        "  train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.1)\n",
        "  print('Training data shape : ', train_X.shape, train_Y.shape)\n",
        "  print('Testing data shape : ', test_X.shape, test_Y.shape)\n",
        "  # In[6]:\n",
        "\n",
        "  plt.figure(figsize=[5,5])\n",
        "\n",
        "  # Display the first image in training data\n",
        "  plt.subplot(121)\n",
        "  plt.imshow(train_X[0,:,:], cmap='gray')\n",
        "  plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
        "\n",
        "  # Display the first image in testing data\n",
        "  plt.subplot(122)\n",
        "  plt.imshow(test_X[0,:,:], cmap='gray')\n",
        "  plt.title(\"Ground Truth : {}\".format(test_Y[0]))\n",
        "\n",
        "\n",
        "\n",
        "  train_X = train_X.astype('float32')\n",
        "  test_X = test_X.astype('float32')\n",
        "  train_X = train_X / 255.\n",
        "  test_X = test_X / 255.\n",
        "  ###aq\n",
        "  batch_size = batch_size\n",
        "  img_height = 150\n",
        "  img_width = 150\n",
        "\n",
        "  data_dir='/content/gdrive/MyDrive/Colab Notebooks/data/entrenamiento'\n",
        "  data_val='/content/gdrive/MyDrive/Colab Notebooks/data/validacion'\n",
        " \n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "  y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "  matc=confusion_matrix(y_real, y_pred)\n",
        "\n",
        "\n",
        "  fig, ax= plt.subplots(figsize=(4,4))\n",
        "  ax.matshow(matc)\n",
        "\n",
        "  plt.title('Confusion matrix CNN+PCA', fontsize=20)\n",
        "  plt.ylabel(\"True labels\", fontsize=15)\n",
        "  plt.xlabel(\"Predicted labels\", fontsize=15)\n",
        "  for(i,j), z in np.ndenumerate(matc):\n",
        "      ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center', color=\"black\", fontsize=15)\n",
        "  plt.imshow(matc, interpolation='nearest', cmap='Pastel1')\n",
        "\n",
        "\n",
        "     \n",
        "  y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "  matc=confusion_matrix(y_real, y_pred)\n",
        "  ax= plt.subplot()\n",
        "  fig, ax= plt.subplots(figsize=(7,6))\n",
        "\n",
        "  sns.heatmap(matc, annot=True, ax = ax); #annot=True to annotate cells\n",
        "\n",
        "  # labels, title and ticks\n",
        "  ax.set_xlabel('Predicción de etiquetas');ax.set_ylabel('Etiquetas reales'); \n",
        "  ax.set_title('Matriz de confusión'); \n",
        "  ax.xaxis.set_ticklabels([\"nail\", \"rectangle\", \"square\", \"washer\", \"circle\", \"nut\"], fontsize=10);\n",
        "  ax.yaxis.set_ticklabels([\"nail\", \"rectangle\", \"square\", \"washer\", \"circle\", \"nut\"], fontsize=10);\n",
        "\n",
        "  report = classification_report(y_real, y_pred, output_dict=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  cm=confusion_matrix(y_real, y_pred)\n",
        "\n",
        "  print(cm)\n",
        "\n",
        "  print( accuracy_score(y_real, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "  #######################:3\n",
        "\n",
        "  n = n+1 #incrementa n para en la siguientre iteracion escriba en el siguiente espacio\n",
        "\n"
      ],
      "id": "b19e49a1"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copia de red_final+pca_reciente.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}